% !TeX root = ../thuthesis-example.tex

% Your chapter title here
% Please make sure that the title is in CAPITALS
% All section and subsection headings, use capital letters where required
\chapter{INTRODUCTION}
\label{introduction}

This chapter gives a brief introduction about the major breakthroughs of the field of deep reinforcement learning starting from a simple DQN \cite{mnih2013playing} to more advanced algorithms such as AlphaZero \cite{silver2017mastering} and AlphaStar \cite{Arulkumaran_2019}. It gives an idea of the complexity of the environments and the challenges these AIs have to overcome and explains why training robust policies is so important for most applications in the real world.    

\section{Major breakthroughs of deep reinforcement learning}
With the advent of deep learning, neural networks have quickly invaded the field of reinforcement learning giving birth to a new branch called deep reinforcement learning (DRL). First steps were taken back in 2013 when \cite{mnih2013playing} used a Deep Q-Network (DQN) to learn to play a number of classic Atari games only by observing their environment from raw pixels using a simple convolutional network. Starting from zero knowledge, the agent explores strategical policies by itself, and after training for a sufficient number of frames, it hopefully learns to play on the target environment, sometimes even achieving super-human performance. Later on, the DRL field had another important breakthrough in 2016, when a famous AI developed by DeepMind known as AlphaGo \cite{alphago} defeated for the first time in the history a 9-dan Go player, Lee Sedol, for 4 games to 1 playing against him without handicaps. The strategy game of GO was, at the time, considered very hard to be solved by AI algorithms due to its enormous number of possible states (about \(10^{170}\)) an agent has to deal with. One year later, its evolution, AlphaZero \cite{silver2017mastering}, even managed to defeat Ke Jie who at the time of the match was ranked first among all human players worldwide. One year later, in 2019, a more general version of AlphaZero, MuZero \cite{Schrittwieser_2020}, successfully mastered the game of Chess, Go, and Shogi without knowing the rules of the games and is even able to generalize to single-agent environments such as Atari games without changing its architecture. In 2018, a new agent developed by Uber, Go-Explore \cite{ecoffet2019goexplore}, for the first time solved the game Montezuma’s Revenge with a score of over 43,000 points, which is almost 4 times the previous state of the art. Montezuma’s Revenge is well known for being a very hard environment for RL agents since its rewards are very sparse, and many tasks have to be done before getting any score. One year later, in 2019, another agent developed by DeepMind, AlphaStar \cite{Arulkumaran_2019}, managed to beat a top-10 player of Starcraft for 5 to 0 as well as defeating 86\% of medium level players. Finally, also in 2019, Tencent MOBA-playing AI system \cite{ye2019mastering} beat 99.81\% of professional human opponents in full 1v1 games of the MOBA \textit{Honor of Kings}. The actor-critic agent included several novel strategies such as control dependency decoupling, action mask, target attention, and dualclip PPO. Master the game of \textit{Honor of Kings} is far more challenging than Go since in a normal match could be encountered about \(10^{600}\) possible states.

\section{Importance of training robust policies}
However, besides performance, safety also plays a fundamental role when deploying DRL-based models in real-world applications. An agent that achieves super-human skills in a certain task but that fails to complete its goal when its inputs are subjected to minimal perturbations can't be considered as a reliable system. Even worse, this lack of robustness may even lead to dangerous situations. In a scenario where an agent driving an autonomous car is fed with adversarial examples, it may be deceived by the environment to choose to take sub-optimal actions, thus degrading its performance. In an even more dangerous scenario, an attacker may craft a sequence of adversarial samples so to lead the agent toward a dangerous target state, thus causing a possible collision or committing an infringement. Besides self-driving cars and videogames, DRL finds application in other critical systems such as training robots used in manufacturing, enhancing the performance of electric power systems, and evaluating trading strategies in the financial sector. Given the critical issues of the above-mentioned scenarios, it is reasonable to think that robustness plays a critical role and these systems should be prevented from being fooled by adversarial observations crafted by a malicious agent.  Hence, the direction of this thesis project concerns studying the effects of adversarial attacks and defenses on deep reinforcement learning algorithms so to evaluate their robustness under different levels of adversary knowledge and their possible defenses, so to make these systems more reliable and usable in the real world.

\section{Thesis overview}
The work in this thesis thus investigates the robustness of some of the most common deep reinforcement learning algorithms and shows their vulnerability both against malicious policies with the goal to mislead the agent toward sub-optimal states and against adversarial attacks aimed at crafting more effective adversarial observations. Malicious or adversarial policies are generated employing policy adversarial attacks which will be introduced in chapter \ref{sec:rla}. Since the evaluated policies take images as input, policy adversarial attacks, in turn, perform image attacks on input observations so to change the victim policy's actions distribution toward an adversarial one. For this reason, we believe that, in order to do a comprehensive study on policies robustness, merely evaluating policy attacks is not enough since these attacks are conditioned by the performance of the image attacks they rely on. Image adversarial attacks will be introduced in chapter \ref{sec:ima}.

However, before going on with the main body of this thesis, it is important to have a clear concept of what is a {\it policy}, an {\it algorithm}, and what we mean for {\it transferability} in our context. A {\it policy} is a model that outputs a probability distribution over actions and it is trained using a reinforcement learning {\it algorithm}. However, two policies trained with the same algorithm may not necessarily be the same policy since a change in the random seed used to train the network implementing each policy would result in its weights having different parameters. Note that two policies trained or equipped with different defense mechanisms are also different policies since the defense method would influence their behavior. Furthermore, we have {\it policy transferability} when adversarial perturbations for the target policy are generated using another policy trained with the same algorithm and for the same task. Similarly, in the case of {\it algorithm transferability}, adversarial perturbations for the target policy are generated using another policy trained with a different algorithm for the same task. This is the same concept of attack transferability on image classification models but applied to the reinforcement learning domain. Moreover, when in the following chapters we are going to refer to a policy trained with algorithm \(A\) on environment \(E\) we are simply going to call it \(E-A\). For instance, Pong-DQN corresponds to a policy trained with DQN on the environment of Pong.

Overall, the main contributions of this work are the following:
\begin{itemize}
    \item \textbf{Study policy attacks transferability over policies and algorithms}: We computed the {\it reward vs attack frequency} curves of different policy attacks under a fixed threat model so to measure their effectiveness and their transferability against policies and algorithms \cite{huang2017adversarial}. We repeated the experiment for a total of 3 algorithms, 5 policy attacks, and 2 environments.
    \item \textbf{Study policy attacks transferability against defended policies}: We computed the {\it reward vs attack frequency} curves of different policy attacks under a fixed threat model so to measure their effectiveness and their transferability against defended policies and defended algorithms. We repeated the experiment for a total of 3 algorithms, 2 policy attacks, and 2 environments.
    \item \textbf{Benchmark of the robustness of defended policies against white-box image attacks methods}: We compared the curves of victim policies' average reward and success rate of different white-box attack methods attacking input observations under a fixed threat model and over increasing values of perturbations' strength \(\epsilon\). Victim policies have also been protected with different defense methods so as to have a better idea of the effectiveness of each attack and the robustness of these defended policies. These curves, introduced in \cite{dong2019benchmarking}, are also known as robustness curves and are used as major evaluation metrics to measure image classification models' robustness.
\end{itemize}

The present thesis work is structured in the following way. Chapter \ref{sec:drl} reviews the policies trained to conduct the experiments and the evaluated environments, chapter \ref{sec:ima} introduces the concept of adversarial attacks and defenses on images and explains some common white-box attacks used as backbone to implement policies adversarial attacks and defenses. Chapter \ref{sec:rla} gives a general overview of adversarial attacks on policies as well as possible defense methods. Chapters \ref{sec:exp1}, \ref{sec:exp2} and \ref{sec:exp3} show, analyze and discuss the results obtained in each of the three experiments respectively. Finally, \ref{sec:conclusions} gives a general discussion and conclusion about the work that has been carried out and points out some future research directions.
      