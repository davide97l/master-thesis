\chapter{Conclusions}
\label{sec:conclusions}

\section{Summary}
In this work, we conducted three experiments to study the robustness of reinforcement learning policies and their defenses. In the first part, we focused on studying policy attacks transferability over policies and algorithms. We did it by computing the {\it reward vs attack frequency} curves of different policy attacks under a fixed threat model so to measure their effectiveness and their transferability against policies and algorithms. We repeated the experiment for a total of 3 algorithms, 5 policy attacks, and 2 environments. In the second experiment, we studied policy attacks transferability against defended policies. We did it by computing the {\it reward vs attack frequency} curves of different policy attacks under a fixed threat model so to measure their effectiveness and their transferability against defended policies and defended algorithms. We repeated the experiment for a total of 3 algorithms, 2 policy attacks, and 2 environments. The third and last experiment consisted of a benchmark to measure the robustness of defended policies against white-box image attacks methods. We compared the curves of victim policies’ average reward and the success rate of different white-box attack methods attacking input observations under a fixed threat model and over increasing values of perturbations’ strength. Victim policies have been protected with different defense methods.

\section{Future research directions}
The present experiment, even if it took a lot of work, can still be expanded to make it more complete. The algorithms that have been examined only include DQN, A2C, and PPO, but many other more or less novel algorithms could be added to this work. For instance, DQN has many more variants worth being tested such as C51-DQN \cite{bellemare2017distributional} and the more powerful Rainbow-DQN \cite{hessel2017rainbow}. Similarly, future work may also introduce new defenses to make the evaluated policies more robust. Many of them combine adversarial training with auxiliary loss functions to train policies with certified robustness \cite{wang2020adversarial} \cite{fischer2019online} \cite{zhang2020robust} \cite{oikarinen2020robust}. The main image attack used as a backbone by the policy attacks is FGSM and attacks relying on the two iterative methods, PGD and MI, have only been partially studied in the last experiment. However, this work could be extended by evaluating more diverse white-box attacks such as C\&W \cite{carlini2016evaluating} and Deepfool \cite{moosavidezfooli2016deepfool}. Instead of only focusing on black-box attacks, it could be interesting to know whether some black-box attacks could further boost the performance of the policy attacks. The only environments that have been explored in this work are simple Atari games with discrete actions. However, in some real scenarios, it is common to have to solve reinforcement learning tasks whose agent has to take continuous actions to interact with the environments. For example, most environments included in the categories MuJoco and Box2D have a continuous actions space which would cause the current implementations of DQN, A2C, and PPO to be useless. Hence, future work might also take into consideration to test the robustness of models specialized to deal with continuous actions environments such as SAC \cite{haarnoja2018soft}, TD3 \cite{fujimoto2018addressing} and DDPG \cite{lillicrap2019continuous}. Another possible scenario is when it is not possible to directly inject noise to the input observations but it is possible to generate adversarial observations by training a malicious agent to interact with the environment in a way such as to fool the victim agent. Under this particular scenario, new kinds of attacks and defenses are possible in order to test the robustness of the target policies.
