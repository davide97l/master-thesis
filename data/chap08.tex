\chapter{DISCUSSIONS}
\label{sec:discussions}

This chapter gives further details about how the work has been carried out and points out some future research directions.
\junz{9 chapters are too many! this chapter can be merged with others, e.g., merge future directions to chap 9, merge details to experiments. }

\section{Number of attacked policies}
One of the major difficulties encountered while carrying out this thesis work is the computation time taken to run all the experiments. For the first experiment have been evaluated 5 policy attacks against 3 reinforcement learning algorithms trained on 2 environments. Each policy has been attacked 4 times corresponding to the four curves shown on each chart: attacking without transferability, attacking a surrogate policy, and attacking two different surrogate algorithms. Each curve has been drawn out of 10 different attack frequencies corresponding to ten points along the \(x\)-axis. Uniform attack, since it is the fastest attack has its corresponding curves containing 20 points. Moreover, each point has been computed as average of 10 runs. Thus, in total have been attacked at least \(5*3*2*4*10*10=12000\) policies. The second experiment instead evaluates 2 policy attacks against 3 reinforcement learning algorithms trained in a single environment. Similarly, each policy has been attacked 4 times corresponding to the four curves shown on each chart as in the previous experiment. Given that each curve is composed of 10 points averaged over 10 runs, in total have been attacked at least \( 2*3*1*4*10*10=2400\) policies. Finally in the third experiment have been evaluated 6 policies trained with DQN where 5 of them were defended with a defense mechanism and trained on the Pong environment. Each policy has been attacked with 3 different white-box attack methods and each curve is composed of 20 points taken over an average of 10 runs. The experiment has been conducted on both targeted and untargeted threat models. Hence, in total have been attacked \(6*3*2*20*10=7200\) policies. Hence, the whole work includes a total of 12000+2400+7200=21600 policy attacks. It took about two months using between 4 and 8 GeForce GTX TITAN X GPUs. In practice, more experiments have been conducted while tuning the attacks' hyper-parameters and verifying the correctness of all the implemented attacks.

\section{Implementation details}
In this section, we are going to discuss the details relative to the implementation of the evaluated reinforcement learning algorithms, the policy attacks, and the image attacks. The Github repository containing the code to obtain the results presented in this thesis is available at the following address: \url{https://github.com/davide97l/rl-policies-attacks-defenses}. It has been developed with Pytorch but relies on other frameworks, each specialized in a different task. The reinforcement learning algorithms have been taken from the framework \href{https://github.com/davide97l/rl-policies-attacks-defenses}{Tianshou} \cite{tianshou} developed by \href{http://ml.cs.tsinghua.edu.cn/}{TSAIL}. It consists of a reinforcement learning platform based on pure PyTorch with support to parallelized environments. Image adversarial attacks have been taken from \href{https://github.com/BorealisAI/advertorch}{Advertorch} \cite{ding2019advertorch} developed by \href{https://www.borealisai.com/en/}{BorealisAI}. Advertorch is a Python toolbox for adversarial robustness research also based on PyTorch. Given that the model interface required by Advertorch is different from the one required by the model trained with Tianshou, has been created an adapter class to adapt Tianshou models to Advertorch models. The two simple image defenses, namely JPEG conversion and feature squeezing have been implemented from scratch. Policy adversarial training has been developed from scratch and integrated into Tianshou. It expands the Tianshou classes implementing the training procedure of the normal policies to defend them with adversarial training. Policy attacks have also been implemented from scratch and made compatible with Tianshou. Their modularized code allows to easily develop new attacks by expanding a base class or any of the existing policies. Hence, the result of this work is also a framework to test the robustness of reinforcement learning trained policies. Finally, the Atari environments have been taken from the Gym library developed by \href{https://openai.com/}{OpenAI}. The same library also includes other well-studied environments made publicly available to the AI community.

\section{Future research directions}
The present experiment, even if it took a lot of work, can still be expanded to make it more complete. The algorithms that have been examined only include DQN, A2C, and PPO, but many other more or less novel algorithms could be added to this work. For instance, DQN has many more variants worth to be tested such as C51-DQN \cite{bellemare2017distributional} and the more powerful Rainbow-DQN \cite{hessel2017rainbow}. Similarly, future work may also introduce new defenses to make the evaluated policies more robust. Many of them combine adversarial training with auxiliary loss functions to train policies with certified robustness \cite{wang2020adversarial} \cite{fischer2019online} \cite{zhang2020robust} \cite{oikarinen2020robust}. The main image attack used as a backbone by the policy attacks is FGSM and attacks relying on the two iterative methods, PGD and MI, have only been partially studied in the last experiment. However, this work could be extended by evaluating more diverse white-box attacks such as C\&W \cite{carlini2016evaluating} and Deepfool \cite{moosavidezfooli2016deepfool}. Instead of only focusing on black-box attacks, it could be interesting to know whether some black-box attacks could further boost the performance of the policy attacks. The only environments that have been explored in this work are simple Atari games with discrete actions. However, in some real scenarios, it is common to have to solve reinforcement learning tasks whose agent has to take continuous actions to interact with the environments. For example, most environments included in the categories MuJoco and Box2D have a continuous actions space which would cause the current implementations of DQN, A2C, and PPO to be useless. Hence, future work might also take into consideration to test the robustness of models specialized to deal with continuous actions environments such as SAC \cite{haarnoja2018soft}, TD3 \cite{fujimoto2018addressing} and DDPG \cite{lillicrap2019continuous}. Another possible scenario is when it is not possible to directly inject noise to the input observations but it is possible to generate adversarial observations by training a malicious agent to interact with the environment in a way such as to fool the victim agent. Under this particular scenario, new kinds of attacks and defenses are possible in order to test the robustness of the target policies.