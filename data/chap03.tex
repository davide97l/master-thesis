% !TeX root = ../thuthesis-example.tex

% Input your chapter title here
\chapter{ADVERSARIAL ATTACKS ON IMAGES}
\label{sec:ima}

Since the policy attacks included in this thesis work by perturbing the image observations given in input to the victim policies, this chapter reviews some of the most common image attacks, their threat model and introduces some common defense methods to counter them.

\section{Adversarial attacks on images}
Since the advent of deep convolutional networks used for image classification \cite{NIPS2012_4824}, the computer vision field has been subjected to many important breakthroughs with the aim to further improve the classification accuracy obtained on many images classification tasks \cite{he2015deep} \cite{zhang2020resnest}. However, despite their high accuracy, those models lack robustness since they can be easily deceived by adversarial examples \cite{goodfellow2014explaining}. More formally, an adversarial example is a \textit{sample input data that has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it} \cite{kurakin2016adversarial}. In fact, to be effective, an adversarial example should be misclassified by deep learning models, but not by the human brain. Therefore, only small changes can be made to the original input image \textit{x} (legitimate example) to craft the adversarial example \(x_{adv}\)=\textit{x}+\(\delta\), where \(\delta\) is also known as adversarial noise. The distance introduced by the noise between \textit{x} and \(x_{adv}\) is usually defined by the \(l_{p}\) norm of the difference between the original and the adversarial sample for some p=0,..,\(\infty\). Hence, an adversarial example \(x_{adv}\) has to satisfy the constraint \(||x_{adv}-x||_p\leq \epsilon\) (adversarial constraint), where smaller \(\epsilon\) values correspond to smaller input perturbations, thus leading to less perceptible changes under the condition that \(x_{adv}\) is misclassified. Moreover, images adversarial attacks can be designed to achieve two different goals:
\begin{itemize}
    \item \textbf{Untargeted}: Untargeted attacks aim at making the model predict any class different from the correct one without targeting at any desired class. Formally speaking, given a classifier \textit{f} and the true label \textit{y}, an adversarial example would cause \(f(x_{adv})\neq y\). Hence, untargeted attacks' goal consists of maximizing the loss of the attacked model respect to the true label \(y\), namely, \(\max \: L(x_{adv}, y)\), under the assumption \(||x_{adv}-x||_p\leq \epsilon\). Given the relaxed constraints the adversarial examples are subjected to, these kinds of attacks are usually easier to perform.
    \item \textbf{Targeted}: Conversely, targeted attacks try to mislead the model's prediction toward a specific class \textit{y'}, that is, \(f(x_{adv})=y'\), where \(y'\neq y\). The loss function can then be formulated as \(\min L(x_{adv}, y')\) always under adversarial constraints. In this way, given the input \(x_{adv}\), it will be more likely that the attacked model would predict the target class \(y'\) rather than any other class. More sophisticated policy attacks usually require crafting adversarial observation in a targeted way so to force the victim agent to perform some required actions.
\end{itemize}
For example, if we want an agent simply to have its performance degraded, attacking under untargeted settings would be enough since preventing it to take the best action would naturally lower its total earned reward. However, if our goal is to control the agent by making it choose some predefined actions such as to lead it to a particular state, then adversarial attacks should be performed under targeted settings. Finally, adversarial attacks are also divided into two main categories depending on how much information is available regarding the model under attack:
\begin{itemize}
    \item \textbf{White-box attacks}: Under this setting, the adversary has full access and knowledge of the model, that is, the architecture of the model, its parameters, gradients, and loss respect to the input as well as possible defense mechanisms are known to the attacker \cite{goodfellow2014explaining}. Thus, it is not particularly difficult to attack models under this condition, and common methods exploit model's output gradients to generate adversarial examples. Only attacks belonging to this category have been evaluated in this work.
    \item \textbf{Black-box attacks}: In this category of attacks, the adversary has zero or very little knowledge about the model. Thus existing methods often rely on training a similar model or an ensemble of them. These methods work because, generally, adversarial examples that fool one model are likely to fool another similar model. Furthermore, in practice, this is the most likely kind of attack, since, in normal circumstances, attackers don't have the possibility to access much of the models' knowledge. Other methods exploit knowledge about the accuracy of the prediction or only the label to craft adversarial examples \cite{wiel2017decisionbased}.
\end{itemize}
Both white and black-box attacks are possible when attacking DRL algorithms and it depends on how much knowledge is known regarding the network defining the policy of the agents. Conversely, adversarial defenses aim to protect deep learning models from adversarial attacks, namely, making models more robust against adversarial examples. In this context, research on adversarial robustness resembles a minimax game where attackers constantly try to exploit more powerful techniques to fool deep learning models while, at the same time, defenders have to invent new defense methods to guard against these malicious attacks.

\subsection{Generating adversarial examples}
In the next sections, have been reported some of the most common white-box attacks on images that are often also used to attack DRL agents. Moreover, these methods can also be used to attack images under black-box settings by crafting adversarial examples attacking similar models and then exploiting the transferability propriety of the adversarial examples to fool the target model \cite{dong2017boosting} or they can be directly applied to perform black-box attacks after estimating gradients by querying the target model \cite{Chen_2017}.

\subsection{FGSM}
Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining} is a basic one-step gradient-based approach that is able to find an adversarial example in a single step by maximizing the loss function \( L(x_{adv}, y)\) with respect to the input \(x\) and then adding back the sign of the output gradients to \(x\) so to produce the adversarial example \(x_{adv}\)
\begin{equation}
x_{adv}=x+\epsilon \cdot sign(\nabla_x L(x, y)),
\end{equation}
where \(\nabla_x L(x, y)\) is the gradient of the loss respect to the input \(x\), and the equation is expected to meet the \(l_{\infty}\) norm bound by design. This method works because adding a perturbation to the legitimate input such to maximize its loss respect to the correct label \textit{y} decreases the likelihood that \textit{y} could be predicted given the input \(x_{adv}\). Mathematically, it moves the adversarial example in one direction toward the border between the true class and some other class \cite{dong2017boosting}. This method is sometimes implemented without the {\it sign} operator (FGM) and it yields similar results to the version with sign \cite{agarwal2018explainable}.

\subsection{I-FGSM}
Basic iterative methods \cite{kurakin2016adversarial} iteratively apply FGSM with a small step size \(\alpha\). Thus, the iterative version of FGSM (I-FGSM) can be expressed as
\begin{equation}
x_{adv}^{t+1}=x_{adv}^{t}+\epsilon \cdot \alpha \cdot sign(\nabla_x L(x_{adv}^{t}, y)),
\end{equation}
where \(x_{adv}^{0}=x\) is the legitimate example. There are several ways to make the adversarial example satisfy the norm bound. For example, \(x_{adv}\) could be clipped into the \(\epsilon\) vicinity of x or set \(\alpha=\epsilon/T\) with \(T\) being the number of iterations. \cite{kurakin2016adversarial} proved that iterative methods exploit much finer perturbations which do not destroy the image even with higher \(\epsilon\) and at the same time confuse the classifier with a higher rate. Their drawback is that iterative methods are a little bit slower than their one-step counterparts.
%, and more importantly, they show poor performance on transferability, which is a fundamental propriety in black-box attacks.

\subsection{MI-FGSM}
Momentum iterative gradient-based methods \cite{dong2017boosting} integrate momentum into iterative fast gradient method to generate adversarial examples satisfying the \(l_{p}\) norm bound. Traditionally, momentum is a technique for accelerating gradient descent algorithms by accumulating a velocity vector in the gradient direction of the loss function across iterations. However, this concept can also be applied to generate adversarial examples and obtain tremendous benefits. As it was for the learning rate update, the first step consists of updating the momentum \(g_t\) by accumulating the velocity vector in the gradient direction as 
\begin{equation}
g_{t+1}=\mu \cdot g_{t} + \frac{\nabla_x L(f(x, y)}{||\nabla_x L(x, y)||_p},
\end{equation}
where \(\mu\) is a decay factor. Next, the adversarial example \(x_{adv}^{t}\) is perturbed in the direction of the sign of \(g_{t}\) with a step size \(\alpha\) as
\begin{equation}
x_{adv}^{t+1}=x_{adv}^{t}+\alpha \cdot sign(g_{t+1}).
\end{equation}
In each iteration, the current gradient \(\nabla_x L(x, y)\) is normalized by the \(l_{p}\) distance of itself because the authors noticed that the scale of the gradients in different iterations varies in magnitude.

\subsection{PGD}
Projected gradient descent (PGD) \cite{madry2019deep} is another iterative algorithm that exploits projected gradient descend to iteratively craft adversarial examples as:
\begin{equation}
x_{adv}^{t+1}=\pi_{x+S}(x_{adv}^{t} +\alpha \cdot sign(\nabla_x L(x_{adv}^{t}, y))),
\end{equation}
where S is the set of all the allowed perturbations. Projected gradient descent performs one step of standard gradient descent, and then clips all the coordinates to be within the \(l_p\) ball. Moreover, in order to explore a large part of the loss landscape, the algorithm is restarted from many points within the \(l_p\) ball around data points taken from the evaluation set. Thanks to this large number of observations, the authors realized that all the local maxima found by PGD have similar loss values, both for normally trained networks and for adversarially trained networks, thus pointing out that robustness against the PGD adversary yields robustness against all first-order adversaries such as SGD based attacks. This conclusion also leads to the fact that as long as the adversary only uses gradients of the loss function with respect to the input, it will not find significantly better local maxima than PGD.

\subsection{C\&W}
The method proposed by Carlini \& Wagner \cite{carlini2016evaluating} relies on the initial formulation of adversarial example and formally defines the problem of finding an adversarial instance for an image \textit{x} as a minimization problem of a continuous function:
\begin{equation} \label{eq:CW}
\min \: ||\delta||_p+c \cdot L(x+\delta, y),
\end{equation}
under the condition that \((x+\delta)\in [0,1]^n\). The function \(f\) is an objective function such that \(L(x+\delta, y)\neq y\) if and only if \(L(x+\delta, y)<=0\). Finally, the term \textit{c} is a suitable chosen hyper-parameter. Moreover, to ensure that the modification yields a valid image, the adversarial noise \(\delta\) is constrained such that \(0\leq x_i+\delta_i\leq 1 \: \forall i\) (here assuming image pixels to be in the range [0,1]). One way to do it, is to replace \((x+\delta)\) with \((1+\tanh(w))/2\) so that the optimization problem in (\ref{eq:CW}) becomes an unconstrained minimization problem in \textit{w}.

\section{Defending against adversarial examples}
Extensive research in developing effective defense mechanisms in order to build robust models and safeguard them against adversarial attacks has also been conducted. One very promising technique is robust training which aims to make a classifier robust against small internal perturbations. Some possible strategies are based on adversarial training by adding generated adversarial examples to the training data \cite{goodfellow2014explaining}, defensive distillation which consists of retraining a network using previously generated soft-labels \cite{papernot2016distillation}, or another technique consists of training robust models with regularization such to train the defended model to ignore small perturbations \cite{hein2017formal}. Another category of defense methods that we are going to examine consists of input transformation. This method is not applied during training but only during inference by transforming the inputs right before feeding them to the classifier with the aim to make adversarial perturbations less effective.

\subsection{Adversarial training}
Adversarial training (AT) is a very simple and intuitive defense method to protect a model against adversarial examples. The first step consists of generating adversarial examples using different attack methods on the target model. In the second step, these adversarial examples are merged to the original training set so to form an augmented training set and finally the target model is retrained on the augmented training set. When adversarial examples are crafted with PGD we have PGD-adversarial training which can train very robust deep networks but it is much more expensive than traditional training due to the iterative design of PGD. In contrast, FGSM-adversarial training is typically faster but less effective \cite{wong2020fast}.

\subsubsection{JPEG compression}
JPEG compression \cite{dziugaite2016study} is a simple input transformation method that converts each input image to JPEG before it is fed to the target network. It has been studied that compressing an image can partially remove possible adversarial perturbations and, at the same time, elude the human perception that no transformation has been applied. However, in practice, this method is not very effective since it degrades the performance of the model while not cleaning completely an image from all the adversarial perturbations. To remedy this problem, \cite{das2018shield} proposes to vaccinate a policy by retraining it on JPEG compressed images multiple times on multiple compression qualities, and use an ensemble of these models to get the final classification label.

\subsubsection{Feature squeezing}
Feature squeezing \cite{Xu_2018} is another input transformation method that reduces the search space available to an adversary by compressing different features vectors in the original space into a single sample. Digital computers usually represent images as an array of pixels each of them representing a specific color (RGB images) or a shadow of grey (greyscale images). Thus, reducing bit depth can reduce the space that an adversarial has to craft perturbations possibly limiting the drop in accuracy that this compression may lead to. For example, an 8-bit greyscale image provides \(2^8=256\) values for each pixel which if reduced to 5-bits we would have only \(2^5=32\) values for each pixel that could be changed to create an adversarial example.